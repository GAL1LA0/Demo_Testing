{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJeBjXw7XxNUSzDiqLy+rs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GAL1LA0/Demo_Testing/blob/main/Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning notebook\n",
        "inspired by Fiebig, Lennart; [youtube tutorial](https://www.youtube.com/watch?v=nsdCRVuprDY); [hugging face](https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt)"
      ],
      "metadata": {
        "id": "Ka9oHwZ59bK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Scrapping from wikipedia for some content for our model. We are scrapping a statical website therefore the scraper is built like the following:\n",
        "For more information regarding dynamical webscrapping please refer to webscraer notebook\n",
        "\n"
      ],
      "metadata": {
        "id": "Vg3IrABaAbW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOMLP0VYTFRV",
        "outputId": "ebed6d3b-65cf-4d96-e414-3b3b00806d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAP S/4HANA aus Wikipedia, der freien EnzyklopÃ¤die Zur Navigation springen Zur Suche springen SAP S/4HANA Basisdaten Entwickler SAP SE Erscheinungsjahr 2015 Aktuelle Version 2023 [1] ( 11. Oktober 2023 ) Betriebssystem Linux Programmiersprache ABAP , C Kategorie ERP Lizenz proprietÃ¤re Lizenz deutschsprachig ja Produktseite SAP S/4HANA ist eine ERP -SoftwarelÃ¶sung der SAP SE und Nachfolger des bisherigen Kernprodukts SAP ECC (SAP E RP C entral C omponent). Das S steht dabei fÃ¼r simple oder suite , die 4 fÃ¼r die vierte Produktgeneration und SAP HANA ( H igh Performance An alytic A ppliance) fÃ¼r die zugrunde liegende Datenbanktechnologie. SAP S/4HANA ist das favorisierte Produkt seitens SAP fÃ¼r den Einsatz in einer Cloud. Inhaltsverzeichnis 1 Verwendung 2 Geschichte 3 Versionen 4 Ãœbersicht der Release-StÃ¤nde (On-Premise und Cloud) 5 Integration in das Unternehmen 6 Implementierung 7 Addons von Drittanbietern 8 Produktlebenszyklus 9 Wartungsende 10 Einzelnachweise Verwendung [ Bearbeiten | Quelltext bearbeiten ] SAP erweitert mit SAP S/4HANA die bestehende ERP -Software-Produktlinie, die alle alltÃ¤glichen Prozesse eines Unternehmens abdecken soll. Die ERP-Software bietet sowohl Funktionen des tÃ¤glichen GeschÃ¤fts als auch IndustrielÃ¶sungen. Ebenfalls sind die typischen SAP-Produkte enthalten (SAP SRM, SAP CRM und SAP SCM). Bestimmte zentrale Funktionen sind bereits im digitalen Kern von SAP S/4HANA enthalten. Andere Funktionen stehen als LoB Solutions (Line of Business Solutions) zur VerfÃ¼gung. Diese LÃ¶sungen bieten einen zusÃ¤tzlichen Funktionsumfang. Sie mÃ¼ssen aber gesondert lizenziert und installiert werden. [2] Die neue SAP Business Suite 4 nutzt die neue SAP-HANA -Datenbank. [3] Daher kommt der Name des Produkts, SAP S/4HANA. [4] Dagegen unterstÃ¼tzen die bisherigen SAP-R/3 -LÃ¶sungen ebenso Datenbanken von Oracle , Microsoft und IBM . [5] Mit diesem Produkt hat SAP die Cloud-Strategie offiziell bestÃ¤tigt und vorangetrieben. [6] Geschichte [ Bearbeiten | Quelltext bearbeiten ] Die erste ProduktprÃ¤sentation erfolgte am 3. Februar 2015 an der New Yorker BÃ¶rse . [7] PrÃ¤sentiert wurden dabei Cloud - und On-Premise -LÃ¶sungen. Cloud-LÃ¶sungen wurden auf der SAPPHIRE (der jÃ¤hrlichen SAP-Kundenkonferenz) am 6. Mai 2015 in Orlando (Florida) zur VerfÃ¼gung gestellt. [8] SAP S/4HANA gilt als eines der grÃ¶ÃŸten Updates in der SAP-ERP-Welt, betreffend Plattform und Strategie. [9] Nebenbei wurden allerdings auch die FunktionalitÃ¤t, die VerfÃ¼gbarkeit, die Preisfestlegung und die Migration rund um SAP S/4HANA in Frage gestellt. [10] Bis zum 21. April 2015 wurde das Produkt 370 Mal verkauft, [11] [12] im dritten Quartal 2015 stieg die Zahl auf 1.300. [13] [14] Bis zum Jahresende 2019 gab es 3.180 Implementierungen. [15] Laut SAP haben sich bis Ende 2020 rund 16.000 Kunden fÃ¼r SAP S/4HANA entschieden, das entspricht einem Anstieg von 2.200 Kunden gegenÃ¼ber dem Vorjahr. [16] Seit 2018 verwendet S/4HANA eine neue technische Basis, die ABAP-Platform und nicht mehr die Basis SAP NetWeaver . [17] Wettbewerbsprodukte zu SAP HANA im Bereich â€ž Software as a Service â€œ (SaaS) werden von den groÃŸen SoftwarehÃ¤usern Oracle , [18] Microsoft , Infor und Workday Inc. [19] angeboten. Versionen [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA existiert in den beiden Versionen On-Premises und Cloud. ZusÃ¤tzlich besteht die MÃ¶glichkeit, SAP S/4HANA in Form eines hybriden Betriebsmodells zu nutzen (Teile On-Premise, Teile in der Cloud). SAP S/4HANA kann als Private Cloud Edition betrieben werden, die die FlexibilitÃ¤t einer klassischen On-Premise-Anwendung mit den geringen Gesamtbetriebskosten eines abonnementbasierten Cloud-ERP-Systems verbindet. [20] Aktuell werden in der Produktfamilie fÃ¼nf verschiedene Optionen unterschieden [21] : Cloud-LÃ¶sungen (Software as a Service oder Software Subscriptions) (RISE with) SAP S/4HANA Cloud (frÃ¼her: SAP S/4HANA Cloud, essentials edition, Multi-Tenant-Edition oder Public Cloud) SAP S/4HANA Cloud, extended edition (frÃ¼her: Single-Tenant Edition oder Private Cloud) RISE with SAP S/4HANA Cloud, private edition (neu 2021) Any-Premise-LÃ¶sungen (auf jeder Infrastruktur) SAP S/4HANA On-Premise managed by SAP (SAP HANA Enterprise Cloud) SAP S/4HANA On-Premise (On-Premise or managed by cloud provider Hyperscalers) Die SAP S/4HANA Cloud steht fÃ¼r spezifische Branchen und 42 LÃ¤nder zur VerfÃ¼gung. Die RISE with SAP S/4HANA Cloud, private edition bietet den gleichen Funktionsumfang wie die On-Premise-LÃ¶sung und steht fÃ¼r 64 LÃ¤nder, in 39 Sprachen und fÃ¼r 25 Branchen zur VerfÃ¼gung. [21] Ãœbersicht der Release-StÃ¤nde (On-Premise und Cloud) [ Bearbeiten | Quelltext bearbeiten ] On-Premise SAP S/4HANA fÃ¼r Finanzen 1503: MÃ¤rz 2015 SAP S/4HANA 1511: 11. November 2015 SAP S/4HANA fÃ¼r Finanzen 1605: Mai 2016 SAP S/4HANA 1610: 31. Oktober 2016 SAP S/4HANA 1709: 15. September 2017 SAP S/4HANA 1809: 21. September 2018 SAP S/4HANA 1909: 20. September 2019 SAP S/4HANA 2020: 7. Oktober 2020 (geÃ¤nderte Nomenklatur [22] ) SAP S/4HANA 2021: 12. Oktober 2021 [23] [24] SAP S/4HANA 2022: 12. Oktober 2022 [25] Cloud [26] SAP S/4HANA Cloud 1908: August 2019 SAP S/4HANA Cloud 1911: November 2019 SAP S/4HANA Cloud 2002: Februar 2020 SAP S/4HANA Cloud 2005: April 2020 SAP S/4HANA Cloud 2008: Juli 2020 SAP S/4HANA Cloud 2011: Oktober 2020 [27] SAP S/4HANA Cloud 2102: Februar 2021 [28] SAP S/4HANA Cloud 2105: Mai 2021 [29] SAP S/4HANA Cloud 2108: August 2021 [30] SAP S/4HANA Cloud 2110: Oktober 2021 [31] SAP S/4HANA Cloud 2202: Januar 2022 [32] SAP S/4HANA Cloud 2208: August 2022 [33] Integration in das Unternehmen [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA kann auf verschiedene Arten in das Unternehmen integriert werden: On-Premise, als Cloud-LÃ¶sung oder als eine Hybrid-Variante in unterschiedlichen AnwendungsfÃ¤llen. [34] Einer der \"kÃ¼rzesten Wege\" [35] zu SAP S/4HANA ist der Weg Ã¼ber SAP Central Finance . Mit Hilfe dieser Plattform (und dem Ansatz \"Finance First\") kÃ¶nnen Unternehmen ihre heterogene Systemlandschaft mit einem zentralisierten SAP S/4HANA-Finance-System verknÃ¼pfen â€“ sowohl SAP-, als auch Non-SAP-Systeme. [36] Implementierung [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA bietet unterschiedliche Arten zur Implementierung (neue Implementierung, System-Konvertierung und selektive Migration). [37] Die richtige Wahl ist stets vom Ausgangspunkt des Kunden abhÃ¤ngig. Dabei mÃ¼ssen verschiedene Kriterien berÃ¼cksichtigt werden. Dazu zÃ¤hlen systemtechnische und organisatorische Einflussfaktoren wie die strategischen Ziele des Unternehmens, die Kosten, die KomplexitÃ¤t der Umsetzung oder die Risikobereitschaft der Organisation. [38] New Implementation [39] (neue Implementierung): wird auch als Greenfield-Ansatz bezeichnet. Darunter versteht man die Migration von einem non-SAP-System oder einem anderen ERP-System auf ein SAP-System. Dieser Vorgang verlangt einen initialen Datenladeprozess. Ziel ist es, die Stamm- und Bewegungsdaten des alten Systems in das neue SAP-System zu migrieren. Die Neuimplementierung erfolgt beim Greenfield-Ansatz stets nah am SAP-Standard und den Best Practices. [2] System Conversion [39] (System-Konvertierung): wird auch als Brownfield -Ansatz bezeichnet. [40] In diesem Szenario hat der Kunde bereits die SAP Business Suite im Einsatz und mÃ¶chte diese nun auf das neue SAP-S/4HANA-Release bringen. Aus der technischen Sicht unterstÃ¼tzen dabei der Software-Update-Manager (SUM) mit der Daten-Migrations-Option (DMO). Selektive Data Transformation [39] (selektive Migration, frÃ¼her: Landscape Transformation): Es gibt SAP Data Management- und Landscape Transformation (DMLT)-Tools und -Services fÃ¼r den selektiven Transfer von Konfiguration und Daten aus dem bisherigen ERP-System in die SAP S/4HANA-Instanz. Dabei kÃ¶nnen mehrere ERP-LÃ¶sungen zusammengefÃ¼hrt werden. [41] Addons von Drittanbietern [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA lÃ¤sst sich auch Ã¼ber Addons von Drittanbietern erweitern. Sowohl die On-premise als auch Cloud-Version (mit Ausnahme der Public-Cloud) lassen sich so um weitere Funktionen und Module erweitern. Dies bietet weitere MÃ¶glichkeiten fÃ¼r die Anwender das System auf ihre BedÃ¼rfnisse anzupassen und ArbeitsablÃ¤ufe und Prozesse zu optimieren. SAP sieht auch eine ZertifizierungsmÃ¶glichkeit fÃ¼r die Addons vor. Produktlebenszyklus [ Bearbeiten | Quelltext bearbeiten ] Beide Versionen von SAP S/4HANA â€“ On-Premise und die Cloud-LÃ¶sung â€“ verfolgen eine quartalsweise Update-Strategie. Dabei wird jedes Quartal ein neues Cloud-Release zur VerfÃ¼gung gestellt, wobei fÃ¼r die On-Premise-Variante nur einmal im Jahr ein neues Release zur VerfÃ¼gung steht. DafÃ¼r werden hier jedes Quartal Feature Pack Stacks (FPS) und/oder Service Pack Stacks (SPS) herausgegeben. On-premise: jÃ¤hrlich neue Produktversion (z.Â B.: SAP S/4HANA 1610), gefolgt von drei FPS â€“ eines pro Quartal. Folgt ein neues Produkt, so bleiben fÃ¼r die VorgÃ¤nger-Varianten die SPS pro Quartal erhalten (bis zum Ende des Supports). Technisch gleicht ein FPS inhaltlich dem SPS; allerdings enthÃ¤lt der FPS weniger â€žbahnbrechendeâ€œ Neuerungen. Die Nummerierung ist aufsteigend â€“ so folgt einem FPS3 ein SPS4. [42] Der erste FPS wurde von SAP fÃ¼r SAP S/4HANA 1610 am 22. Februar 2017 verÃ¶ffentlicht. [43] Cloud: SAP bietet fÃ¼r die Cloud-LÃ¶sung fÃ¼r alle produktiven Kunden ein Update pro Quartal. Dabei kÃ¶nnen die Updates neue Business-Funktionen und/oder Fehlerbehebungen beinhalten. Kleine ErgÃ¤nzungen oder Fehlerbehebungen werden in Bug-Fixes oder Patches ausgeliefert. [44] Wartungsende [ Bearbeiten | Quelltext bearbeiten ] Im Februar 2020 hat Christian Klein , damals als Co-Vorstandssprecher der SAP, den Kunden eine Wartungszusage fÃ¼r S/4HANA bis Ende 2040 gegeben. [45] Einzelnachweise [ Bearbeiten | Quelltext bearbeiten ] â†‘ blogs.sap.com . â†‘ a b 15 Fragen zu SAP S/4HANA. Abgerufen am 12.Â August 2020 . â†‘ S/4 Hana lÃ¶st SAP Business Suite ab. isreport.de, 4.Â Februar 2015, abgerufen am 23.Â Juli 2018 (deutsch). â†‘ S/4HANA â€“ What Procurement Teams Should be Doing Now. UpperEdge.com, 26.Â August 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ The SAPÂ® Business Suite 4 SAP HANAÂ® (SAP S/4HANA) FAQ. Bluefin Solutions, 3.Â Februar 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Ravi Padmanabhan: What Is SAP S/4 HANA Cloud? Velocity Technology Solutions, 8.Â MÃ¤rz 2017, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Katherine Noyes: SAP unwraps a new enterprise suite based on Hana. PCWorld, 3.Â Februar 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Joey Jackson: SAP launches S/4 HANA cloud edition. RCRWirless News, 6.Â Mai 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ All Eyes on SAP S/4HANA and Cloud in SAP's Q1 Results. Forbes, 20.Â April 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ SAP S/4HANA Is a Transformational Shift for SAP and Its Users, but Hold on to Your Wallets for Now. Gartner, 24.Â Februar 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Kelsey Mason: SAP starts seeing the HANA adoption needed to drive long-term growth. TBR Newsroom, 21.Â April 2015, archiviert vom Original am 4.Â MÃ¤rz 2016 ; abgerufen am 23.Â Juli 2018 (englisch). â†‘ Aaron Ricadela: SAP Second-Quarter Earnings Tempered by Cloud Demand. Bloomberg, 21.Â Juli 2015, archiviert vom Original am 17.Â November 2015 ; abgerufen am 23.Â Juli 2018 (englisch). â†‘ Aaron Ricadela: SAP Outpaces Rivals, Sees Robust Pickup for New Software. Bloomberg, 20.Â Oktober 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Brian McKenna: SAP Q3 2015 results: On-premise and cloud grow in parallel, says Cohen. ComputerWeekly, 20.Â Oktober 2015, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Softwarekonzern: Warum so viele SAP-Kunden bei S/4 Hana noch zÃ¶gern. Abgerufen am 5.Â Februar 2020 . â†‘ SAP 2020 Integrierter Bericht. Abgerufen am 16.Â MÃ¤rz 2021 . â†‘ ABAP Platform â€“ Part 1 â€“ Evolution from SAP Netweaver | SAP Blogs. Abgerufen am 13.Â MÃ¤rz 2023 . â†‘ Toby Wolpe: Oracleâ€™s in-memory option aims to beat the rest within 12 months. ZDNet, 10.Â Juni 2014, abgerufen am 23.Â Juli 2018 (englisch). â†‘ JP Mangalindan: Why Workday has Oracle and SAP worried. Fortune, 12.Â MÃ¤rz 2012, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Maren Szydlowski: 8 GrÃ¼nde fÃ¼r die SAP S/4HANA Private Cloud Edition. Cpro Projects & Solutions GmbH, 12.Â Februar 2021, abgerufen am 17.Â MÃ¤rz 2021 . â†‘ a b Arnin Hoque: SAP S/4HANA Cloud and On-Premise Deployment Options. SAP SE, 29.Â Mai 2020, abgerufen am 19.Â August 2020 (englisch). â†‘ Deciphering the World of SAP S/4HANA | SAP Blogs. Abgerufen am 24.Â September 2020 . â†‘ SAP S/4HANA 2021 Release Highlights in Sales | SAP Blogs. Abgerufen am 13.Â Oktober 2021 . â†‘ SAP S/4HANA Cloud and SAP S/4HANA 2021 Product Release | SAP Blogs. Abgerufen am 15.Â Oktober 2021 . â†‘ 2022 Release Highlights in Seconds: SAP S/4HANA & SAP S/4HANA Cloud, private edition | SAP Blogs. Abgerufen am 6.Â Dezember 2022 . â†‘ SAP S/4HANA Cloud, Public Edition - New Product Releases. Abgerufen am 6.Â Dezember 2022 . â†‘ Schedules & Updates for your SAP Global Certification. Abgerufen am 24.Â September 2020 . â†‘ SAP S/4HANA Cloud Release. Abgerufen am 9.Â MÃ¤rz 2021 (englisch). â†‘ SAP S/4HANA Cloud 2105 Release | SAP Blogs. Abgerufen am 21.Â September 2021 . â†‘ New Product Releases | SAP S/4HANA Cloud | SAP Community. Abgerufen am 21.Â September 2021 . â†‘ Highlights of the SAP S/4HANA Cloud 2111 Release | SAP Blogs. Abgerufen am 18.Â Dezember 2021 . â†‘ Highlights of the SAP S/4HANA Cloud 2202 Release | SAP Blogs. Abgerufen am 6.Â Dezember 2022 . â†‘ Highlights of the SAP S/4HANA Cloud 2208 Release | SAP Blogs. Abgerufen am 6.Â Dezember 2022 . â†‘ Transitioning to SAP S/4 HANA â€“ Choosing a deployment Option. SAPGurus, 23.Â November 2016, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Der kÃ¼rzeste Weg zu SAP S/4HANA | Central Finance. 22.Â Januar 2019, abgerufen am 4.Â MÃ¤rz 2021 (deutsch). â†‘ Central Finance. Abgerufen am 4.Â MÃ¤rz 2021 . â†‘ Frank Densborn: How do you Migrate to SAP S/4HANA. SAP SE, 21.Â Juni 2016, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Thomas Pasquale: Wege zur SAP-Roadmap. E-3 Magazin, abgerufen am 12.Â August 2020 . â†‘ a b c How to Move to SAP S/4HANA | SAP Blogs. Abgerufen am 12.Â August 2020 . â†‘ Greenfield oder Brownfield? Die SAP S/4HANA StrategieansÃ¤tze im Vergleich. In: IT und SAP Blog abilis GmbH IT-Services & Consulting. 18.Â Oktober 2018, abgerufen am 30.Â Mai 2019 (deutsch). â†‘ Wechseln Sie mit dem hybriden Ansatz zu SAP S/4HANA. itelligence AG, abgerufen am 12.Â August 2020 . â†‘ Yannick Peterschmitt: SAP S/4HANA on premise edition: FPS versus SPS. SAP SE, 8.Â April 2016, abgerufen am 23.Â Juli 2018 (englisch). â†‘ Rudolf Hois: SAP S/4HANA, on-premise edition 1610: Feature Pack Stack 1 (FPS01). SAP SE, 22.Â Februar 2017, abgerufen am 23.Â Juli 2018 (englisch). â†‘ SAP Release and Maintenance Strategy. (PDF, 2,5 MB) SAP SE, 29.Â MÃ¤rz 2017, abgerufen am 23.Â Juli 2018 (englisch). (Support-Zugang erforderlich) â†‘ SAP nimmt viel Druck aus dem Kessel. Abgerufen am 5.Â Februar 2020 . Abgerufen von â€ž https://de.wikipedia.org/w/index.php?title=SAP_S/4HANA&oldid=238331149 â€œ Kategorien : SAP Data-Warehousing NavigationsmenÃ¼ Meine Werkzeuge Nicht angemeldet Diskussionsseite BeitrÃ¤ge Benutzerkonto erstellen Anmelden NamensrÃ¤ume Artikel Diskussion Deutsch Ansichten Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Weitere Suche Navigation Hauptseite Themenportale ZufÃ¤lliger Artikel Mitmachen Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Ã„nderungen Kontakt Spenden Werkzeuge Links auf diese Seite Ã„nderungen an verlinkten Seiten Spezialseiten Permanenter Link SeitenÂ­Â­informationen Artikel zitieren Kurzlink Wikidata-Datenobjekt Drucken/â€‹exportieren Buch erstellen Als PDF herunterladen Druckversion In anderen Sprachen English FranÃ§ais æ—¥æœ¬èªž í•œêµ­ì–´ ä¸­æ–‡ Links bearbeiten Diese Seite wurde zuletzt am 20. Oktober 2023 um 13:32 Uhr bearbeitet. Abrufstatistik Â· Autoren Der Text ist unter der Lizenz â€žCreative-Commons Namensnennung â€“ Weitergabe unter gleichen Bedingungenâ€œ verfÃ¼gbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) kÃ¶nnen im Regelfall durch Anklicken dieser abgerufen werden. MÃ¶glicherweise unterliegen die Inhalte jeweils zusÃ¤tzlichen Bedingungen. Durch die Nutzung dieser Website erklÃ¤ren Sie sich mit den Nutzungsbedingungen und der Datenschutzrichtlinie einverstanden. WikipediaÂ® ist eine eingetragene Marke der Wikimedia Foundation Inc. Datenschutz Ãœber Wikipedia Impressum Verhaltenskodex Entwickler Statistiken Stellungnahme zu Cookies Mobile Ansicht\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import bs4\n",
        "\n",
        "response = requests.get('https://de.wikipedia.org/wiki/SAP_S/4HANA',headers={'User-Agent': 'Mozilla/5.0'})# deutsch englisch egal?\n",
        "soup = bs4.BeautifulSoup(response.text,features=\"html.parser\")\n",
        "\n",
        "#soup.body.get_text(' ', strip=True)\n",
        "generated_Text = soup.body.get_text(' ', strip=True)\n",
        "print(generated_Text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   First try with gpt2. Here I tried to fine-tune the gpt2 model with the help of masked language model (AutoModelForMaskedLM) approach. However, this approach only works with certain models, such as [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_brmMhIEA2Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize input text\n",
        "input_text = generated_Text\n",
        "tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Instantiate the pre-trained GPT-2 model for masked language modeling\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Forward pass with the tokenized input through the model\n",
        "outputs = model(**tokens)\n",
        "\n",
        "# Access the model's output\n",
        "logits = outputs.logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "mp569Ev3e0O6",
        "outputId": "7da4ba6c-468e-47e7-a5c8-11ebedfa7bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5846 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e3741598403e>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Instantiate the pre-trained GPT-2 model for masked language modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Forward pass with the tokenized input through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             )\n\u001b[0;32m--> 474\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XmodConfig, YosoConfig."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model_checkpoint = \"gpt2\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "EoDCnWmebGNM",
        "outputId": "14406e77-9f78-4268-d511-75f97b538c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b0265a025323>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             )\n\u001b[0;32m--> 474\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XmodConfig, YosoConfig."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "f_nYAsA0fWp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Second try with gpt2 model: In this approach we try to train the model with our scraped text using [hugging face](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer) Trainer-class\n",
        "*   At first we importe the necessery packages, prepare the toknizer and specify the model we want to use\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNAQO1P8jVuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "import torch"
      ],
      "metadata": {
        "id": "clvE3HU2jb2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.save_pretrained(\"/content/gpt2_tokenizer\")\n",
        "\n",
        "# Confirm that the tokenizer is saved in the Colab environment\n",
        "!ls /content/gpt2_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34YJXceljjG6",
        "outputId": "6936f127-b62f-4970-9fcc-15cf688b7538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\tmerges.txt\t   special_tokens_map.json  vocab.json\n",
            "generation_config.json\tmodel.safetensors  tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.save_pretrained(\"/content/gpt2_tokenizer\")\n",
        "!ls /content/gpt2_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGgl1xa6nwWO",
        "outputId": "cae61e42-dfbe-480e-e076-1cf1d0b05b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\tmerges.txt\t   pytorch_model.bin\t    tokenizer_config.json\n",
            "generation_config.json\tmodel.safetensors  special_tokens_map.json  vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   trying to generate a text before training the model and observing the result\n"
      ],
      "metadata": {
        "id": "dVIIL-emCi4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(\n",
        "    tokenizer.encode(\"Das S steht dabei\", return_tensors='pt'),\n",
        "    do_sample=True,\n",
        "    max_length = 30,\n",
        "    pad_token_id = model.config.eos_token_id,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(tokenizer.decode(output[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xspT18noLQN",
        "outputId": "d2e564fb-fcf5-46e2-9225-00680efbce6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Das S steht dabei, saal aieleit hir wert daat daat saam, nee kut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   putting the text in a txt file for the further usage/simulating a training corpus\n",
        "*  Utilizing the load_dataset and load_data_collator to load/prepare the dataset for training. These methods are going to be used in the train method, which is responsible for the actual training of the model\n",
        "\n"
      ],
      "metadata": {
        "id": "W3kN6jlNCtQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/wikipedia.txt\"\n",
        "\n",
        "with open(file_name, \"w\") as file:\n",
        "    file.write(generated_Text)\n",
        "\n",
        "!ls /content\n",
        "#from google.colab import files\n",
        "#files.download(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPEcgzW4sJEc",
        "outputId": "2eccc04e-d435-4477-e575-96f2dd3ce04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cached_lm_GPT2Tokenizer_128_wikipedia.txt\tgpt2_tokenizer\tsample_data\n",
            "cached_lm_GPT2Tokenizer_128_wikipedia.txt.lock\toutput_dir\twikipedia.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "veYnsGEPo-g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator"
      ],
      "metadata": {
        "id": "bSfjE08eqXjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "ugVNeKTtqXpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip show accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydvcAqOCvVKP",
        "outputId": "91a33ab6-e0e9-4bda-d128-afb7f396964c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.28.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Name: accelerate\n",
            "Version: 0.25.0\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: sylvain@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    train_file_path='/content/wikipedia.txt',\n",
        "    model_name='gpt2',\n",
        "    output_dir='/content/output_dir',\n",
        "    overwrite_output_dir=False,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=50.0,\n",
        "    save_steps=50000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "VXrw11d3qbFV",
        "outputId": "8af81d04-baee-4c5f-f8c0-413dee360637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5/300 00:53 < 1:27:45, 0.06 it/s, Epoch 0.67/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7f00c12341ba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/wikipedia.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/output_dir'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-da01640259cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_file_path, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps)\u001b[0m\n\u001b[1;32m     29\u001b[0m   )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m         )\n\u001b[0;32m-> 1662\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1663\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2717\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89e6PWI8qcb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **logits, etc below are for mlm - next step?** - can be ignored. maybe useful for the further development"
      ],
      "metadata": {
        "id": "fxW5MB7wnxEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#def Text_spliter(Text_webscraper):\n",
        "  # Split the text into chunks\n",
        "chunk_size = 512\n",
        "text_chunks = [input_text[i:i + chunk_size] for i in range(0, len(input_text), chunk_size)]\n",
        "\n",
        "  # Tokenize each chunk separately\n",
        "all_tokens = []\n",
        "for chunk in text_chunks:\n",
        "  tokens = tokenizer(chunk, return_tensors=\"pt\", truncation=True)\n",
        "  all_tokens.append(tokens)\n",
        "\n",
        "  # Concatenate the tokenized chunks\n",
        "combined_tokens = {key: torch.cat([t[key] for t in all_tokens], dim=1) for key in all_tokens[0].keys()}\n",
        "\n",
        "  # Forward pass with the tokenized input through the model\n",
        "outputs = model(**combined_tokens)\n",
        "  #return outputs"
      ],
      "metadata": {
        "id": "G6FjvE59kvCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(generated_Text, return_tensors=\"pt\")\n",
        "output = model(**tokens)\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "id": "L-Zb99jPjwXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Instantiate the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Your long text\n",
        "long_text = generated_Text\n",
        "\n",
        "# Tokenize the long text\n",
        "tokens = tokenizer(long_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "# Extract the input_ids from the tokens\n",
        "input_ids = tokens[\"input_ids\"]\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 512\n",
        "\n",
        "# Split the input_ids into chunks\n",
        "chunks = [input_ids[:, i:i+chunk_size] for i in range(0, input_ids.shape[1], chunk_size)]\n",
        "\n",
        "# Forward pass through the model for each chunk\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    # Perform the forward pass\n",
        "    outputs = model(**{\"input_ids\": chunk})\n",
        "\n",
        "    # Access the model's output (adjust this based on your specific model and task)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # You can do further processing with the logits or other model outputs\n",
        "\n",
        "    # Print information about the chunk and its output\n",
        "    print(f\"Chunk {idx} length: {chunk.shape[1]}\")\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "\n",
        "# Now you can use the model outputs as needed\n"
      ],
      "metadata": {
        "id": "M1rnbZLTrg_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}