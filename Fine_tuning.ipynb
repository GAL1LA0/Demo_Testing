{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJeBjXw7XxNUSzDiqLy+rs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GAL1LA0/Demo_Testing/blob/main/Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning notebook\n",
        "inspired by Fiebig, Lennart; [youtube tutorial](https://www.youtube.com/watch?v=nsdCRVuprDY); [hugging face](https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt)"
      ],
      "metadata": {
        "id": "Ka9oHwZ59bK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Scrapping from wikipedia for some content for our model. We are scrapping a statical website therefore the scraper is built like the following:\n",
        "For more information regarding dynamical webscrapping please refer to webscraer notebook\n",
        "\n"
      ],
      "metadata": {
        "id": "Vg3IrABaAbW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOMLP0VYTFRV",
        "outputId": "ebed6d3b-65cf-4d96-e414-3b3b00806d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAP S/4HANA aus Wikipedia, der freien Enzyklopädie Zur Navigation springen Zur Suche springen SAP S/4HANA Basisdaten Entwickler SAP SE Erscheinungsjahr 2015 Aktuelle Version 2023 [1] ( 11. Oktober 2023 ) Betriebssystem Linux Programmiersprache ABAP , C Kategorie ERP Lizenz proprietäre Lizenz deutschsprachig ja Produktseite SAP S/4HANA ist eine ERP -Softwarelösung der SAP SE und Nachfolger des bisherigen Kernprodukts SAP ECC (SAP E RP C entral C omponent). Das S steht dabei für simple oder suite , die 4 für die vierte Produktgeneration und SAP HANA ( H igh Performance An alytic A ppliance) für die zugrunde liegende Datenbanktechnologie. SAP S/4HANA ist das favorisierte Produkt seitens SAP für den Einsatz in einer Cloud. Inhaltsverzeichnis 1 Verwendung 2 Geschichte 3 Versionen 4 Übersicht der Release-Stände (On-Premise und Cloud) 5 Integration in das Unternehmen 6 Implementierung 7 Addons von Drittanbietern 8 Produktlebenszyklus 9 Wartungsende 10 Einzelnachweise Verwendung [ Bearbeiten | Quelltext bearbeiten ] SAP erweitert mit SAP S/4HANA die bestehende ERP -Software-Produktlinie, die alle alltäglichen Prozesse eines Unternehmens abdecken soll. Die ERP-Software bietet sowohl Funktionen des täglichen Geschäfts als auch Industrielösungen. Ebenfalls sind die typischen SAP-Produkte enthalten (SAP SRM, SAP CRM und SAP SCM). Bestimmte zentrale Funktionen sind bereits im digitalen Kern von SAP S/4HANA enthalten. Andere Funktionen stehen als LoB Solutions (Line of Business Solutions) zur Verfügung. Diese Lösungen bieten einen zusätzlichen Funktionsumfang. Sie müssen aber gesondert lizenziert und installiert werden. [2] Die neue SAP Business Suite 4 nutzt die neue SAP-HANA -Datenbank. [3] Daher kommt der Name des Produkts, SAP S/4HANA. [4] Dagegen unterstützen die bisherigen SAP-R/3 -Lösungen ebenso Datenbanken von Oracle , Microsoft und IBM . [5] Mit diesem Produkt hat SAP die Cloud-Strategie offiziell bestätigt und vorangetrieben. [6] Geschichte [ Bearbeiten | Quelltext bearbeiten ] Die erste Produktpräsentation erfolgte am 3. Februar 2015 an der New Yorker Börse . [7] Präsentiert wurden dabei Cloud - und On-Premise -Lösungen. Cloud-Lösungen wurden auf der SAPPHIRE (der jährlichen SAP-Kundenkonferenz) am 6. Mai 2015 in Orlando (Florida) zur Verfügung gestellt. [8] SAP S/4HANA gilt als eines der größten Updates in der SAP-ERP-Welt, betreffend Plattform und Strategie. [9] Nebenbei wurden allerdings auch die Funktionalität, die Verfügbarkeit, die Preisfestlegung und die Migration rund um SAP S/4HANA in Frage gestellt. [10] Bis zum 21. April 2015 wurde das Produkt 370 Mal verkauft, [11] [12] im dritten Quartal 2015 stieg die Zahl auf 1.300. [13] [14] Bis zum Jahresende 2019 gab es 3.180 Implementierungen. [15] Laut SAP haben sich bis Ende 2020 rund 16.000 Kunden für SAP S/4HANA entschieden, das entspricht einem Anstieg von 2.200 Kunden gegenüber dem Vorjahr. [16] Seit 2018 verwendet S/4HANA eine neue technische Basis, die ABAP-Platform und nicht mehr die Basis SAP NetWeaver . [17] Wettbewerbsprodukte zu SAP HANA im Bereich „ Software as a Service “ (SaaS) werden von den großen Softwarehäusern Oracle , [18] Microsoft , Infor und Workday Inc. [19] angeboten. Versionen [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA existiert in den beiden Versionen On-Premises und Cloud. Zusätzlich besteht die Möglichkeit, SAP S/4HANA in Form eines hybriden Betriebsmodells zu nutzen (Teile On-Premise, Teile in der Cloud). SAP S/4HANA kann als Private Cloud Edition betrieben werden, die die Flexibilität einer klassischen On-Premise-Anwendung mit den geringen Gesamtbetriebskosten eines abonnementbasierten Cloud-ERP-Systems verbindet. [20] Aktuell werden in der Produktfamilie fünf verschiedene Optionen unterschieden [21] : Cloud-Lösungen (Software as a Service oder Software Subscriptions) (RISE with) SAP S/4HANA Cloud (früher: SAP S/4HANA Cloud, essentials edition, Multi-Tenant-Edition oder Public Cloud) SAP S/4HANA Cloud, extended edition (früher: Single-Tenant Edition oder Private Cloud) RISE with SAP S/4HANA Cloud, private edition (neu 2021) Any-Premise-Lösungen (auf jeder Infrastruktur) SAP S/4HANA On-Premise managed by SAP (SAP HANA Enterprise Cloud) SAP S/4HANA On-Premise (On-Premise or managed by cloud provider Hyperscalers) Die SAP S/4HANA Cloud steht für spezifische Branchen und 42 Länder zur Verfügung. Die RISE with SAP S/4HANA Cloud, private edition bietet den gleichen Funktionsumfang wie die On-Premise-Lösung und steht für 64 Länder, in 39 Sprachen und für 25 Branchen zur Verfügung. [21] Übersicht der Release-Stände (On-Premise und Cloud) [ Bearbeiten | Quelltext bearbeiten ] On-Premise SAP S/4HANA für Finanzen 1503: März 2015 SAP S/4HANA 1511: 11. November 2015 SAP S/4HANA für Finanzen 1605: Mai 2016 SAP S/4HANA 1610: 31. Oktober 2016 SAP S/4HANA 1709: 15. September 2017 SAP S/4HANA 1809: 21. September 2018 SAP S/4HANA 1909: 20. September 2019 SAP S/4HANA 2020: 7. Oktober 2020 (geänderte Nomenklatur [22] ) SAP S/4HANA 2021: 12. Oktober 2021 [23] [24] SAP S/4HANA 2022: 12. Oktober 2022 [25] Cloud [26] SAP S/4HANA Cloud 1908: August 2019 SAP S/4HANA Cloud 1911: November 2019 SAP S/4HANA Cloud 2002: Februar 2020 SAP S/4HANA Cloud 2005: April 2020 SAP S/4HANA Cloud 2008: Juli 2020 SAP S/4HANA Cloud 2011: Oktober 2020 [27] SAP S/4HANA Cloud 2102: Februar 2021 [28] SAP S/4HANA Cloud 2105: Mai 2021 [29] SAP S/4HANA Cloud 2108: August 2021 [30] SAP S/4HANA Cloud 2110: Oktober 2021 [31] SAP S/4HANA Cloud 2202: Januar 2022 [32] SAP S/4HANA Cloud 2208: August 2022 [33] Integration in das Unternehmen [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA kann auf verschiedene Arten in das Unternehmen integriert werden: On-Premise, als Cloud-Lösung oder als eine Hybrid-Variante in unterschiedlichen Anwendungsfällen. [34] Einer der \"kürzesten Wege\" [35] zu SAP S/4HANA ist der Weg über SAP Central Finance . Mit Hilfe dieser Plattform (und dem Ansatz \"Finance First\") können Unternehmen ihre heterogene Systemlandschaft mit einem zentralisierten SAP S/4HANA-Finance-System verknüpfen – sowohl SAP-, als auch Non-SAP-Systeme. [36] Implementierung [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA bietet unterschiedliche Arten zur Implementierung (neue Implementierung, System-Konvertierung und selektive Migration). [37] Die richtige Wahl ist stets vom Ausgangspunkt des Kunden abhängig. Dabei müssen verschiedene Kriterien berücksichtigt werden. Dazu zählen systemtechnische und organisatorische Einflussfaktoren wie die strategischen Ziele des Unternehmens, die Kosten, die Komplexität der Umsetzung oder die Risikobereitschaft der Organisation. [38] New Implementation [39] (neue Implementierung): wird auch als Greenfield-Ansatz bezeichnet. Darunter versteht man die Migration von einem non-SAP-System oder einem anderen ERP-System auf ein SAP-System. Dieser Vorgang verlangt einen initialen Datenladeprozess. Ziel ist es, die Stamm- und Bewegungsdaten des alten Systems in das neue SAP-System zu migrieren. Die Neuimplementierung erfolgt beim Greenfield-Ansatz stets nah am SAP-Standard und den Best Practices. [2] System Conversion [39] (System-Konvertierung): wird auch als Brownfield -Ansatz bezeichnet. [40] In diesem Szenario hat der Kunde bereits die SAP Business Suite im Einsatz und möchte diese nun auf das neue SAP-S/4HANA-Release bringen. Aus der technischen Sicht unterstützen dabei der Software-Update-Manager (SUM) mit der Daten-Migrations-Option (DMO). Selektive Data Transformation [39] (selektive Migration, früher: Landscape Transformation): Es gibt SAP Data Management- und Landscape Transformation (DMLT)-Tools und -Services für den selektiven Transfer von Konfiguration und Daten aus dem bisherigen ERP-System in die SAP S/4HANA-Instanz. Dabei können mehrere ERP-Lösungen zusammengeführt werden. [41] Addons von Drittanbietern [ Bearbeiten | Quelltext bearbeiten ] SAP S/4HANA lässt sich auch über Addons von Drittanbietern erweitern. Sowohl die On-premise als auch Cloud-Version (mit Ausnahme der Public-Cloud) lassen sich so um weitere Funktionen und Module erweitern. Dies bietet weitere Möglichkeiten für die Anwender das System auf ihre Bedürfnisse anzupassen und Arbeitsabläufe und Prozesse zu optimieren. SAP sieht auch eine Zertifizierungsmöglichkeit für die Addons vor. Produktlebenszyklus [ Bearbeiten | Quelltext bearbeiten ] Beide Versionen von SAP S/4HANA – On-Premise und die Cloud-Lösung – verfolgen eine quartalsweise Update-Strategie. Dabei wird jedes Quartal ein neues Cloud-Release zur Verfügung gestellt, wobei für die On-Premise-Variante nur einmal im Jahr ein neues Release zur Verfügung steht. Dafür werden hier jedes Quartal Feature Pack Stacks (FPS) und/oder Service Pack Stacks (SPS) herausgegeben. On-premise: jährlich neue Produktversion (z. B.: SAP S/4HANA 1610), gefolgt von drei FPS – eines pro Quartal. Folgt ein neues Produkt, so bleiben für die Vorgänger-Varianten die SPS pro Quartal erhalten (bis zum Ende des Supports). Technisch gleicht ein FPS inhaltlich dem SPS; allerdings enthält der FPS weniger „bahnbrechende“ Neuerungen. Die Nummerierung ist aufsteigend – so folgt einem FPS3 ein SPS4. [42] Der erste FPS wurde von SAP für SAP S/4HANA 1610 am 22. Februar 2017 veröffentlicht. [43] Cloud: SAP bietet für die Cloud-Lösung für alle produktiven Kunden ein Update pro Quartal. Dabei können die Updates neue Business-Funktionen und/oder Fehlerbehebungen beinhalten. Kleine Ergänzungen oder Fehlerbehebungen werden in Bug-Fixes oder Patches ausgeliefert. [44] Wartungsende [ Bearbeiten | Quelltext bearbeiten ] Im Februar 2020 hat Christian Klein , damals als Co-Vorstandssprecher der SAP, den Kunden eine Wartungszusage für S/4HANA bis Ende 2040 gegeben. [45] Einzelnachweise [ Bearbeiten | Quelltext bearbeiten ] ↑ blogs.sap.com . ↑ a b 15 Fragen zu SAP S/4HANA. Abgerufen am 12. August 2020 . ↑ S/4 Hana löst SAP Business Suite ab. isreport.de, 4. Februar 2015, abgerufen am 23. Juli 2018 (deutsch). ↑ S/4HANA – What Procurement Teams Should be Doing Now. UpperEdge.com, 26. August 2015, abgerufen am 23. Juli 2018 (englisch). ↑ The SAP® Business Suite 4 SAP HANA® (SAP S/4HANA) FAQ. Bluefin Solutions, 3. Februar 2015, abgerufen am 23. Juli 2018 (englisch). ↑ Ravi Padmanabhan: What Is SAP S/4 HANA Cloud? Velocity Technology Solutions, 8. März 2017, abgerufen am 23. Juli 2018 (englisch). ↑ Katherine Noyes: SAP unwraps a new enterprise suite based on Hana. PCWorld, 3. Februar 2015, abgerufen am 23. Juli 2018 (englisch). ↑ Joey Jackson: SAP launches S/4 HANA cloud edition. RCRWirless News, 6. Mai 2015, abgerufen am 23. Juli 2018 (englisch). ↑ All Eyes on SAP S/4HANA and Cloud in SAP's Q1 Results. Forbes, 20. April 2015, abgerufen am 23. Juli 2018 (englisch). ↑ SAP S/4HANA Is a Transformational Shift for SAP and Its Users, but Hold on to Your Wallets for Now. Gartner, 24. Februar 2015, abgerufen am 23. Juli 2018 (englisch). ↑ Kelsey Mason: SAP starts seeing the HANA adoption needed to drive long-term growth. TBR Newsroom, 21. April 2015, archiviert vom Original am 4. März 2016 ; abgerufen am 23. Juli 2018 (englisch). ↑ Aaron Ricadela: SAP Second-Quarter Earnings Tempered by Cloud Demand. Bloomberg, 21. Juli 2015, archiviert vom Original am 17. November 2015 ; abgerufen am 23. Juli 2018 (englisch). ↑ Aaron Ricadela: SAP Outpaces Rivals, Sees Robust Pickup for New Software. Bloomberg, 20. Oktober 2015, abgerufen am 23. Juli 2018 (englisch). ↑ Brian McKenna: SAP Q3 2015 results: On-premise and cloud grow in parallel, says Cohen. ComputerWeekly, 20. Oktober 2015, abgerufen am 23. Juli 2018 (englisch). ↑ Softwarekonzern: Warum so viele SAP-Kunden bei S/4 Hana noch zögern. Abgerufen am 5. Februar 2020 . ↑ SAP 2020 Integrierter Bericht. Abgerufen am 16. März 2021 . ↑ ABAP Platform – Part 1 – Evolution from SAP Netweaver | SAP Blogs. Abgerufen am 13. März 2023 . ↑ Toby Wolpe: Oracle’s in-memory option aims to beat the rest within 12 months. ZDNet, 10. Juni 2014, abgerufen am 23. Juli 2018 (englisch). ↑ JP Mangalindan: Why Workday has Oracle and SAP worried. Fortune, 12. März 2012, abgerufen am 23. Juli 2018 (englisch). ↑ Maren Szydlowski: 8 Gründe für die SAP S/4HANA Private Cloud Edition. Cpro Projects & Solutions GmbH, 12. Februar 2021, abgerufen am 17. März 2021 . ↑ a b Arnin Hoque: SAP S/4HANA Cloud and On-Premise Deployment Options. SAP SE, 29. Mai 2020, abgerufen am 19. August 2020 (englisch). ↑ Deciphering the World of SAP S/4HANA | SAP Blogs. Abgerufen am 24. September 2020 . ↑ SAP S/4HANA 2021 Release Highlights in Sales | SAP Blogs. Abgerufen am 13. Oktober 2021 . ↑ SAP S/4HANA Cloud and SAP S/4HANA 2021 Product Release | SAP Blogs. Abgerufen am 15. Oktober 2021 . ↑ 2022 Release Highlights in Seconds: SAP S/4HANA & SAP S/4HANA Cloud, private edition | SAP Blogs. Abgerufen am 6. Dezember 2022 . ↑ SAP S/4HANA Cloud, Public Edition - New Product Releases. Abgerufen am 6. Dezember 2022 . ↑ Schedules & Updates for your SAP Global Certification. Abgerufen am 24. September 2020 . ↑ SAP S/4HANA Cloud Release. Abgerufen am 9. März 2021 (englisch). ↑ SAP S/4HANA Cloud 2105 Release | SAP Blogs. Abgerufen am 21. September 2021 . ↑ New Product Releases | SAP S/4HANA Cloud | SAP Community. Abgerufen am 21. September 2021 . ↑ Highlights of the SAP S/4HANA Cloud 2111 Release | SAP Blogs. Abgerufen am 18. Dezember 2021 . ↑ Highlights of the SAP S/4HANA Cloud 2202 Release | SAP Blogs. Abgerufen am 6. Dezember 2022 . ↑ Highlights of the SAP S/4HANA Cloud 2208 Release | SAP Blogs. Abgerufen am 6. Dezember 2022 . ↑ Transitioning to SAP S/4 HANA – Choosing a deployment Option. SAPGurus, 23. November 2016, abgerufen am 23. Juli 2018 (englisch). ↑ Der kürzeste Weg zu SAP S/4HANA | Central Finance. 22. Januar 2019, abgerufen am 4. März 2021 (deutsch). ↑ Central Finance. Abgerufen am 4. März 2021 . ↑ Frank Densborn: How do you Migrate to SAP S/4HANA. SAP SE, 21. Juni 2016, abgerufen am 23. Juli 2018 (englisch). ↑ Thomas Pasquale: Wege zur SAP-Roadmap. E-3 Magazin, abgerufen am 12. August 2020 . ↑ a b c How to Move to SAP S/4HANA | SAP Blogs. Abgerufen am 12. August 2020 . ↑ Greenfield oder Brownfield? Die SAP S/4HANA Strategieansätze im Vergleich. In: IT und SAP Blog abilis GmbH IT-Services & Consulting. 18. Oktober 2018, abgerufen am 30. Mai 2019 (deutsch). ↑ Wechseln Sie mit dem hybriden Ansatz zu SAP S/4HANA. itelligence AG, abgerufen am 12. August 2020 . ↑ Yannick Peterschmitt: SAP S/4HANA on premise edition: FPS versus SPS. SAP SE, 8. April 2016, abgerufen am 23. Juli 2018 (englisch). ↑ Rudolf Hois: SAP S/4HANA, on-premise edition 1610: Feature Pack Stack 1 (FPS01). SAP SE, 22. Februar 2017, abgerufen am 23. Juli 2018 (englisch). ↑ SAP Release and Maintenance Strategy. (PDF, 2,5 MB) SAP SE, 29. März 2017, abgerufen am 23. Juli 2018 (englisch). (Support-Zugang erforderlich) ↑ SAP nimmt viel Druck aus dem Kessel. Abgerufen am 5. Februar 2020 . Abgerufen von „ https://de.wikipedia.org/w/index.php?title=SAP_S/4HANA&oldid=238331149 “ Kategorien : SAP Data-Warehousing Navigationsmenü Meine Werkzeuge Nicht angemeldet Diskussionsseite Beiträge Benutzerkonto erstellen Anmelden Namensräume Artikel Diskussion Deutsch Ansichten Lesen Bearbeiten Quelltext bearbeiten Versionsgeschichte Weitere Suche Navigation Hauptseite Themenportale Zufälliger Artikel Mitmachen Artikel verbessern Neuen Artikel anlegen Autorenportal Hilfe Letzte Änderungen Kontakt Spenden Werkzeuge Links auf diese Seite Änderungen an verlinkten Seiten Spezialseiten Permanenter Link Seiten­­informationen Artikel zitieren Kurzlink Wikidata-Datenobjekt Drucken/​exportieren Buch erstellen Als PDF herunterladen Druckversion In anderen Sprachen English Français 日本語 한국어 中文 Links bearbeiten Diese Seite wurde zuletzt am 20. Oktober 2023 um 13:32 Uhr bearbeitet. Abrufstatistik · Autoren Der Text ist unter der Lizenz „Creative-Commons Namensnennung – Weitergabe unter gleichen Bedingungen“ verfügbar; Informationen zu den Urhebern und zum Lizenzstatus eingebundener Mediendateien (etwa Bilder oder Videos) können im Regelfall durch Anklicken dieser abgerufen werden. Möglicherweise unterliegen die Inhalte jeweils zusätzlichen Bedingungen. Durch die Nutzung dieser Website erklären Sie sich mit den Nutzungsbedingungen und der Datenschutzrichtlinie einverstanden. Wikipedia® ist eine eingetragene Marke der Wikimedia Foundation Inc. Datenschutz Über Wikipedia Impressum Verhaltenskodex Entwickler Statistiken Stellungnahme zu Cookies Mobile Ansicht\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import bs4\n",
        "\n",
        "response = requests.get('https://de.wikipedia.org/wiki/SAP_S/4HANA',headers={'User-Agent': 'Mozilla/5.0'})# deutsch englisch egal?\n",
        "soup = bs4.BeautifulSoup(response.text,features=\"html.parser\")\n",
        "\n",
        "#soup.body.get_text(' ', strip=True)\n",
        "generated_Text = soup.body.get_text(' ', strip=True)\n",
        "print(generated_Text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   First try with gpt2. Here I tried to fine-tune the gpt2 model with the help of masked language model (AutoModelForMaskedLM) approach. However, this approach only works with certain models, such as [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_brmMhIEA2Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize input text\n",
        "input_text = generated_Text\n",
        "tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Instantiate the pre-trained GPT-2 model for masked language modeling\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Forward pass with the tokenized input through the model\n",
        "outputs = model(**tokens)\n",
        "\n",
        "# Access the model's output\n",
        "logits = outputs.logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "mp569Ev3e0O6",
        "outputId": "7da4ba6c-468e-47e7-a5c8-11ebedfa7bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5846 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e3741598403e>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Instantiate the pre-trained GPT-2 model for masked language modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Forward pass with the tokenized input through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             )\n\u001b[0;32m--> 474\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XmodConfig, YosoConfig."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model_checkpoint = \"gpt2\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "EoDCnWmebGNM",
        "outputId": "14406e77-9f78-4268-d511-75f97b538c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b0265a025323>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             )\n\u001b[0;32m--> 474\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, EsmConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegaConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XmodConfig, YosoConfig."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "f_nYAsA0fWp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Second try with gpt2 model: In this approach we try to train the model with our scraped text using [hugging face](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer) Trainer-class\n",
        "*   At first we importe the necessery packages, prepare the toknizer and specify the model we want to use\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNAQO1P8jVuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "import torch"
      ],
      "metadata": {
        "id": "clvE3HU2jb2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.save_pretrained(\"/content/gpt2_tokenizer\")\n",
        "\n",
        "# Confirm that the tokenizer is saved in the Colab environment\n",
        "!ls /content/gpt2_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34YJXceljjG6",
        "outputId": "6936f127-b62f-4970-9fcc-15cf688b7538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\tmerges.txt\t   special_tokens_map.json  vocab.json\n",
            "generation_config.json\tmodel.safetensors  tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.save_pretrained(\"/content/gpt2_tokenizer\")\n",
        "!ls /content/gpt2_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGgl1xa6nwWO",
        "outputId": "cae61e42-dfbe-480e-e076-1cf1d0b05b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\tmerges.txt\t   pytorch_model.bin\t    tokenizer_config.json\n",
            "generation_config.json\tmodel.safetensors  special_tokens_map.json  vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   trying to generate a text before training the model and observing the result\n"
      ],
      "metadata": {
        "id": "dVIIL-emCi4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(\n",
        "    tokenizer.encode(\"Das S steht dabei\", return_tensors='pt'),\n",
        "    do_sample=True,\n",
        "    max_length = 30,\n",
        "    pad_token_id = model.config.eos_token_id,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(tokenizer.decode(output[0],skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xspT18noLQN",
        "outputId": "d2e564fb-fcf5-46e2-9225-00680efbce6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Das S steht dabei, saal aieleit hir wert daat daat saam, nee kut\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   putting the text in a txt file for the further usage/simulating a training corpus\n",
        "*  Utilizing the load_dataset and load_data_collator to load/prepare the dataset for training. These methods are going to be used in the train method, which is responsible for the actual training of the model\n",
        "\n"
      ],
      "metadata": {
        "id": "W3kN6jlNCtQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/wikipedia.txt\"\n",
        "\n",
        "with open(file_name, \"w\") as file:\n",
        "    file.write(generated_Text)\n",
        "\n",
        "!ls /content\n",
        "#from google.colab import files\n",
        "#files.download(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPEcgzW4sJEc",
        "outputId": "2eccc04e-d435-4477-e575-96f2dd3ce04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cached_lm_GPT2Tokenizer_128_wikipedia.txt\tgpt2_tokenizer\tsample_data\n",
            "cached_lm_GPT2Tokenizer_128_wikipedia.txt.lock\toutput_dir\twikipedia.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "veYnsGEPo-g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator"
      ],
      "metadata": {
        "id": "bSfjE08eqXjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "ugVNeKTtqXpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip show accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydvcAqOCvVKP",
        "outputId": "91a33ab6-e0e9-4bda-d128-afb7f396964c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.28.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Name: accelerate\n",
            "Version: 0.25.0\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: sylvain@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    train_file_path='/content/wikipedia.txt',\n",
        "    model_name='gpt2',\n",
        "    output_dir='/content/output_dir',\n",
        "    overwrite_output_dir=False,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=50.0,\n",
        "    save_steps=50000\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "VXrw11d3qbFV",
        "outputId": "8af81d04-baee-4c5f-f8c0-413dee360637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  5/300 00:53 < 1:27:45, 0.06 it/s, Epoch 0.67/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7f00c12341ba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/wikipedia.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/output_dir'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-da01640259cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_file_path, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps)\u001b[0m\n\u001b[1;32m     29\u001b[0m   )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1660\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m         )\n\u001b[0;32m-> 1662\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1663\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1929\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2717\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89e6PWI8qcb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **logits, etc below are for mlm - next step?** - can be ignored. maybe useful for the further development"
      ],
      "metadata": {
        "id": "fxW5MB7wnxEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#def Text_spliter(Text_webscraper):\n",
        "  # Split the text into chunks\n",
        "chunk_size = 512\n",
        "text_chunks = [input_text[i:i + chunk_size] for i in range(0, len(input_text), chunk_size)]\n",
        "\n",
        "  # Tokenize each chunk separately\n",
        "all_tokens = []\n",
        "for chunk in text_chunks:\n",
        "  tokens = tokenizer(chunk, return_tensors=\"pt\", truncation=True)\n",
        "  all_tokens.append(tokens)\n",
        "\n",
        "  # Concatenate the tokenized chunks\n",
        "combined_tokens = {key: torch.cat([t[key] for t in all_tokens], dim=1) for key in all_tokens[0].keys()}\n",
        "\n",
        "  # Forward pass with the tokenized input through the model\n",
        "outputs = model(**combined_tokens)\n",
        "  #return outputs"
      ],
      "metadata": {
        "id": "G6FjvE59kvCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(generated_Text, return_tensors=\"pt\")\n",
        "output = model(**tokens)\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "id": "L-Zb99jPjwXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Instantiate the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Your long text\n",
        "long_text = generated_Text\n",
        "\n",
        "# Tokenize the long text\n",
        "tokens = tokenizer(long_text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "# Extract the input_ids from the tokens\n",
        "input_ids = tokens[\"input_ids\"]\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 512\n",
        "\n",
        "# Split the input_ids into chunks\n",
        "chunks = [input_ids[:, i:i+chunk_size] for i in range(0, input_ids.shape[1], chunk_size)]\n",
        "\n",
        "# Forward pass through the model for each chunk\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    # Perform the forward pass\n",
        "    outputs = model(**{\"input_ids\": chunk})\n",
        "\n",
        "    # Access the model's output (adjust this based on your specific model and task)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # You can do further processing with the logits or other model outputs\n",
        "\n",
        "    # Print information about the chunk and its output\n",
        "    print(f\"Chunk {idx} length: {chunk.shape[1]}\")\n",
        "    print(f\"Logits shape: {logits.shape}\")\n",
        "\n",
        "# Now you can use the model outputs as needed\n"
      ],
      "metadata": {
        "id": "M1rnbZLTrg_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}